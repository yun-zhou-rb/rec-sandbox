{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR: Bayesian Personalized Ranking from Implicit Feedback\n",
    "\n",
    "Ref: \n",
    "* https://arxiv.org/pdf/1205.2618\n",
    "* https://medium.com/radon-dev/implicit-bayesian-personalized-ranking-in-tensorflow-b4dfa733c478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# LOAD AND PREPARE THE DATA\n",
    "#---------------------------\n",
    "\n",
    "# Load the dataframe from a tab separated file.\n",
    "df = pd.read_csv('data/movielens/ml-20m/ratings.csv', sep=',')\n",
    "    \n",
    "# Add column names\n",
    "df = df.drop(df.columns[3], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        2     3.5\n",
       "1       1       29     3.5\n",
       "2       1       32     3.5\n",
       "3       1       47     3.5\n",
       "4       1       50     3.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie = pd.read_csv('data/movielens/ml-20m/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#users: 138,493, #items: 26,744\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop any rows with 0 rating\n",
    "df = df.loc[df.rating != 0]\n",
    "\n",
    "# Convert movies into numerical IDs\n",
    "df['user_id'] = df['userId'].astype(\"category\").cat.codes\n",
    "df['movie_id'] = df['movieId'].astype(\"category\").cat.codes\n",
    "\n",
    "# Create a lookup frame so we can get the movie\n",
    "# names back in readable form later.\n",
    "item_lookup = df[['movie_id', 'movieId']].drop_duplicates()\n",
    "item_lookup['movie_id'] = item_lookup.movie_id.astype(str)\n",
    "\n",
    "# We drop our old user and item columns\n",
    "df = df.drop(['userId', 'movieId'], axis=1)\n",
    "\n",
    "# Drop any rows with 0 rating\n",
    "df = df.loc[df.rating != 0]\n",
    "\n",
    "# Create lists of all users, movies and ratings\n",
    "users = list(np.sort(df.user_id.unique()))\n",
    "movies = list(np.sort(df.movie_id.unique()))\n",
    "ratings = list(df.rating)\n",
    "print(f\"#users: {len(users):,}, #items: {len(movies):,}\")\n",
    "\n",
    "# Get the rows and columns for our new matrix\n",
    "rows = df.user_id.astype(float)\n",
    "cols = df.movie_id.astype(float)\n",
    "\n",
    "# Contruct a sparse matrix for our users and items containing number of ratings\n",
    "data_sparse = sp.csr_matrix((ratings, (rows, cols)), shape=(len(users), len(movies)))\n",
    "\n",
    "# Get the values of our matrix as a list of user ids\n",
    "# and item ids. Note that our litsts have the same length\n",
    "# as each user id repeats one time for each rated movie.\n",
    "uids, iids = data_sparse.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "# HYPERPARAMS\n",
    "#-------------\n",
    "\n",
    "epochs = 50\n",
    "batches = 30\n",
    "num_factors = 64 # Number of latent features\n",
    "\n",
    "# Independent lambda regularization values \n",
    "# for user, items and bias.\n",
    "lambda_user = 0.0000001\n",
    "lambda_item = 0.0000001\n",
    "lambda_bias = 0.0000001\n",
    "\n",
    "# Our learning rate \n",
    "lr = 0.005\n",
    "\n",
    "# How many (u,i,j) triplets we sample for each batch\n",
    "samples = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# TENSORFLOW GRAPH\n",
    "#-------------------------\n",
    "\n",
    "# Set up our Tensorflow graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "def init_variable(size, dim, name=None):\n",
    "    '''\n",
    "    Helper function to initialize a new variable with\n",
    "    uniform random values.\n",
    "    '''\n",
    "    std = np.sqrt(2 / dim)\n",
    "    return tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n",
    "\n",
    "\n",
    "def embed(inputs, size, dim, name=None):\n",
    "    '''\n",
    "    Helper function to get a Tensorflow variable and create\n",
    "    an embedding lookup to map our user and item\n",
    "    indices to vectors.\n",
    "    '''\n",
    "    emb = init_variable(size, dim, name)\n",
    "    return tf.nn.embedding_lookup(emb, inputs)\n",
    "\n",
    "\n",
    "def get_variable(graph, session, name):\n",
    "    '''\n",
    "    Helper function to get the value of a\n",
    "    Tensorflow variable by name.\n",
    "    '''\n",
    "    v = graph.get_operation_by_name(name)\n",
    "    v = v.values()[0]\n",
    "    v = v.eval(session=session)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 09:05:41.277662 4481074624 deprecation.py:323] From <ipython-input-9-6d216b63fecb>:44: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    '''\n",
    "    Loss function: \n",
    "    -SUM ln σ(xui - xuj) + λ(w1)**2 + λ(w2)**2 + λ(w3)**2 ...\n",
    "    ln = the natural log\n",
    "    σ(xuij) = the sigmoid function of xuij.\n",
    "    λ = lambda regularization value.\n",
    "    ||W||**2 = the squared L2 norm of our model parameters.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Input into our model, in this case our user (u),\n",
    "    # known item (i) an unknown item (i) triplets.\n",
    "    u = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    i = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    j = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "\n",
    "    # User feature embedding\n",
    "    u_factors = embed(u, len(users), num_factors, 'user_factors') # U matrix\n",
    "\n",
    "    # Known and unknown item embeddings\n",
    "    item_factors = init_variable(len(movies), num_factors, \"item_factors\") # V matrix\n",
    "    i_factors = tf.nn.embedding_lookup(item_factors, i)\n",
    "    j_factors = tf.nn.embedding_lookup(item_factors, j)\n",
    "\n",
    "    # i and j bias embeddings.\n",
    "    item_bias = init_variable(len(movies), 1, \"item_bias\")\n",
    "    i_bias = tf.nn.embedding_lookup(item_bias, i)\n",
    "    i_bias = tf.reshape(i_bias, [-1, 1])\n",
    "    j_bias = tf.nn.embedding_lookup(item_bias, j)\n",
    "    j_bias = tf.reshape(j_bias, [-1, 1])\n",
    "\n",
    "    # Calculate the dot product + bias for known and unknown\n",
    "    # item to get xui and xuj.\n",
    "    xui = i_bias + tf.reduce_sum(u_factors * i_factors, axis=2)\n",
    "    xuj = j_bias + tf.reduce_sum(u_factors * j_factors, axis=2)\n",
    "\n",
    "    # We calculate xuij.\n",
    "    xuij = xui - xuj\n",
    "\n",
    "    # Calculate the mean AUC (area under curve).\n",
    "    # if xuij is greater than 0, that means that \n",
    "    # xui is greater than xuj (and thats what we want).\n",
    "    u_auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "    # Output the AUC value to tensorboard for monitoring.\n",
    "    tf.summary.scalar('auc', u_auc)\n",
    "\n",
    "    # Calculate the squared L2 norm ||W||**2 multiplied by λ.\n",
    "    l2_norm = tf.add_n([\n",
    "        lambda_user * tf.reduce_sum(tf.multiply(u_factors, u_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(i_factors, i_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(j_factors, j_factors)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(i_bias, i_bias)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(j_bias, j_bias))\n",
    "        ])\n",
    "\n",
    "    # Calculate the loss as ||W||**2 - ln σ(Xuij)\n",
    "    #loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij)))\n",
    "    loss = -tf.reduce_mean(tf.log(tf.sigmoid(xuij))) + l2_norm\n",
    "    \n",
    "    # Train using the Adam optimizer to minimize \n",
    "    # our loss function.\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    step = opt.minimize(loss)\n",
    "\n",
    "    # Initialize all tensorflow variables.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 30/1500 [00:02<01:59, 12.29it/s]\u001b[A\n",
      "Loss: 0.764 | AUC: 0.559:   2%|▏         | 30/1500 [00:02<01:59, 12.29it/s]\u001b[A\n",
      "Loss: 0.764 | AUC: 0.559:   4%|▍         | 60/1500 [00:04<01:58, 12.20it/s]\u001b[A\n",
      "Loss: 0.689 | AUC: 0.609:   4%|▍         | 60/1500 [00:04<01:58, 12.20it/s]\u001b[A\n",
      "Loss: 0.689 | AUC: 0.609:   6%|▌         | 90/1500 [00:07<01:55, 12.16it/s]\u001b[A\n",
      "Loss: 0.621 | AUC: 0.660:   6%|▌         | 90/1500 [00:07<01:55, 12.16it/s]\u001b[A\n",
      "Loss: 0.621 | AUC: 0.660:   8%|▊         | 120/1500 [00:10<01:54, 12.00it/s]\u001b[A\n",
      "Loss: 0.554 | AUC: 0.712:   8%|▊         | 120/1500 [00:10<01:54, 12.00it/s]\u001b[A\n",
      "Loss: 0.554 | AUC: 0.712:  10%|█         | 150/1500 [00:12<01:54, 11.75it/s]\u001b[A\n",
      "Loss: 0.470 | AUC: 0.776:  10%|█         | 150/1500 [00:12<01:54, 11.75it/s]\u001b[A\n",
      "Loss: 0.470 | AUC: 0.776:  12%|█▏        | 180/1500 [00:15<01:54, 11.55it/s]\u001b[A\n",
      "Loss: 0.356 | AUC: 0.866:  12%|█▏        | 180/1500 [00:15<01:54, 11.55it/s]\u001b[A\n",
      "Loss: 0.356 | AUC: 0.866:  14%|█▍        | 210/1500 [00:17<01:50, 11.68it/s]\u001b[A\n",
      "Loss: 0.257 | AUC: 0.920:  14%|█▍        | 210/1500 [00:17<01:50, 11.68it/s]\u001b[A\n",
      "Loss: 0.257 | AUC: 0.920:  16%|█▌        | 240/1500 [00:20<01:48, 11.59it/s]\u001b[A\n",
      "Loss: 0.208 | AUC: 0.933:  16%|█▌        | 240/1500 [00:20<01:48, 11.59it/s]\u001b[A\n",
      "Loss: 0.208 | AUC: 0.933:  18%|█▊        | 270/1500 [00:23<01:45, 11.64it/s]\u001b[A\n",
      "Loss: 0.178 | AUC: 0.945:  18%|█▊        | 270/1500 [00:23<01:45, 11.64it/s]\u001b[A\n",
      "Loss: 0.178 | AUC: 0.945:  20%|██        | 300/1500 [00:25<01:42, 11.76it/s]\u001b[A\n",
      "Loss: 0.171 | AUC: 0.946:  20%|██        | 300/1500 [00:25<01:42, 11.76it/s]\u001b[A\n",
      "Loss: 0.171 | AUC: 0.946:  22%|██▏       | 330/1500 [00:28<01:38, 11.87it/s]\u001b[A\n",
      "Loss: 0.163 | AUC: 0.947:  22%|██▏       | 330/1500 [00:28<01:38, 11.87it/s]\u001b[A\n",
      "Loss: 0.163 | AUC: 0.947:  24%|██▍       | 360/1500 [00:30<01:37, 11.71it/s]\u001b[A\n",
      "Loss: 0.154 | AUC: 0.951:  24%|██▍       | 360/1500 [00:30<01:37, 11.71it/s]\u001b[A\n",
      "Loss: 0.154 | AUC: 0.951:  26%|██▌       | 390/1500 [00:33<01:35, 11.68it/s]\u001b[A\n",
      "Loss: 0.152 | AUC: 0.951:  26%|██▌       | 390/1500 [00:33<01:35, 11.68it/s]\u001b[A\n",
      "Loss: 0.152 | AUC: 0.951:  28%|██▊       | 420/1500 [00:35<01:32, 11.72it/s]\u001b[A\n",
      "Loss: 0.148 | AUC: 0.953:  28%|██▊       | 420/1500 [00:35<01:32, 11.72it/s]\u001b[A\n",
      "Loss: 0.148 | AUC: 0.953:  30%|███       | 450/1500 [00:38<01:28, 11.82it/s]\u001b[A\n",
      "Loss: 0.150 | AUC: 0.950:  30%|███       | 450/1500 [00:38<01:28, 11.82it/s]\u001b[A\n",
      "Loss: 0.150 | AUC: 0.950:  32%|███▏      | 480/1500 [00:40<01:26, 11.80it/s]\u001b[A\n",
      "Loss: 0.144 | AUC: 0.952:  32%|███▏      | 480/1500 [00:40<01:26, 11.80it/s]\u001b[A\n",
      "Loss: 0.144 | AUC: 0.952:  34%|███▍      | 510/1500 [00:43<01:23, 11.82it/s]\u001b[A\n",
      "Loss: 0.142 | AUC: 0.954:  34%|███▍      | 510/1500 [00:43<01:23, 11.82it/s]\u001b[A\n",
      "Loss: 0.142 | AUC: 0.954:  36%|███▌      | 540/1500 [00:45<01:20, 11.87it/s]\u001b[A\n",
      "Loss: 0.144 | AUC: 0.952:  36%|███▌      | 540/1500 [00:45<01:20, 11.87it/s]\u001b[A\n",
      "Loss: 0.144 | AUC: 0.952:  38%|███▊      | 570/1500 [00:48<01:19, 11.69it/s]\u001b[A\n",
      "Loss: 0.139 | AUC: 0.954:  38%|███▊      | 570/1500 [00:48<01:19, 11.69it/s]\u001b[A\n",
      "Loss: 0.139 | AUC: 0.954:  40%|████      | 600/1500 [00:51<01:16, 11.75it/s]\u001b[A\n",
      "Loss: 0.133 | AUC: 0.956:  40%|████      | 600/1500 [00:51<01:16, 11.75it/s]\u001b[A\n",
      "Loss: 0.133 | AUC: 0.956:  42%|████▏     | 630/1500 [00:53<01:15, 11.56it/s]\u001b[A\n",
      "Loss: 0.134 | AUC: 0.956:  42%|████▏     | 630/1500 [00:53<01:15, 11.56it/s]\u001b[A\n",
      "Loss: 0.134 | AUC: 0.956:  44%|████▍     | 660/1500 [00:56<01:13, 11.43it/s]\u001b[A\n",
      "Loss: 0.135 | AUC: 0.954:  44%|████▍     | 660/1500 [00:56<01:13, 11.43it/s]\u001b[A\n",
      "Loss: 0.135 | AUC: 0.954:  46%|████▌     | 690/1500 [00:59<01:13, 11.08it/s]\u001b[A\n",
      "Loss: 0.134 | AUC: 0.955:  46%|████▌     | 690/1500 [00:59<01:13, 11.08it/s]\u001b[A\n",
      "Loss: 0.134 | AUC: 0.955:  48%|████▊     | 720/1500 [01:01<01:09, 11.24it/s]\u001b[A\n",
      "Loss: 0.131 | AUC: 0.957:  48%|████▊     | 720/1500 [01:01<01:09, 11.24it/s]\u001b[A\n",
      "Loss: 0.131 | AUC: 0.957:  50%|█████     | 750/1500 [01:04<01:05, 11.45it/s]\u001b[A\n",
      "Loss: 0.125 | AUC: 0.958:  50%|█████     | 750/1500 [01:04<01:05, 11.45it/s]\u001b[A\n",
      "Loss: 0.125 | AUC: 0.958:  52%|█████▏    | 780/1500 [01:07<01:02, 11.49it/s]\u001b[A\n",
      "Loss: 0.131 | AUC: 0.956:  52%|█████▏    | 780/1500 [01:07<01:02, 11.49it/s]\u001b[A\n",
      "Loss: 0.131 | AUC: 0.956:  54%|█████▍    | 810/1500 [01:09<01:00, 11.49it/s]\u001b[A\n",
      "Loss: 0.124 | AUC: 0.958:  54%|█████▍    | 810/1500 [01:09<01:00, 11.49it/s]\u001b[A\n",
      "Loss: 0.124 | AUC: 0.958:  56%|█████▌    | 840/1500 [01:12<00:58, 11.36it/s]\u001b[A\n",
      "Loss: 0.125 | AUC: 0.957:  56%|█████▌    | 840/1500 [01:12<00:58, 11.36it/s]\u001b[A\n",
      "Loss: 0.125 | AUC: 0.957:  58%|█████▊    | 870/1500 [06:48<35:55,  3.42s/it]\u001b[A\n",
      "Loss: 0.127 | AUC: 0.959:  58%|█████▊    | 870/1500 [06:48<35:55,  3.42s/it]\u001b[A\n",
      "Loss: 0.127 | AUC: 0.959:  60%|██████    | 900/1500 [06:53<24:27,  2.45s/it]\u001b[A\n",
      "Loss: 0.124 | AUC: 0.959:  60%|██████    | 900/1500 [06:53<24:27,  2.45s/it]\u001b[A\n",
      "Loss: 0.124 | AUC: 0.959:  62%|██████▏   | 930/1500 [06:58<16:45,  1.76s/it]\u001b[A\n",
      "Loss: 0.125 | AUC: 0.959:  62%|██████▏   | 930/1500 [06:58<16:45,  1.76s/it]\u001b[A\n",
      "Loss: 0.125 | AUC: 0.959:  64%|██████▍   | 960/1500 [07:00<11:20,  1.26s/it]\u001b[A\n",
      "Loss: 0.121 | AUC: 0.960:  64%|██████▍   | 960/1500 [07:00<11:20,  1.26s/it]\u001b[A\n",
      "Loss: 0.121 | AUC: 0.960:  66%|██████▌   | 990/1500 [07:03<07:41,  1.10it/s]\u001b[A\n",
      "Loss: 0.119 | AUC: 0.962:  66%|██████▌   | 990/1500 [07:03<07:41,  1.10it/s]\u001b[A\n",
      "Loss: 0.119 | AUC: 0.962:  68%|██████▊   | 1020/1500 [07:05<05:16,  1.52it/s]\u001b[A\n",
      "Loss: 0.116 | AUC: 0.963:  68%|██████▊   | 1020/1500 [07:05<05:16,  1.52it/s]\u001b[A\n",
      "Loss: 0.116 | AUC: 0.963:  70%|███████   | 1050/1500 [07:08<03:37,  2.06it/s]\u001b[A\n",
      "Loss: 0.115 | AUC: 0.963:  70%|███████   | 1050/1500 [07:08<03:37,  2.06it/s]\u001b[A\n",
      "Loss: 0.115 | AUC: 0.963:  72%|███████▏  | 1080/1500 [07:10<02:32,  2.76it/s]\u001b[A\n",
      "Loss: 0.116 | AUC: 0.963:  72%|███████▏  | 1080/1500 [07:10<02:32,  2.76it/s]\u001b[A\n",
      "Loss: 0.116 | AUC: 0.963:  74%|███████▍  | 1110/1500 [07:12<01:48,  3.61it/s]\u001b[A\n",
      "Loss: 0.122 | AUC: 0.959:  74%|███████▍  | 1110/1500 [07:12<01:48,  3.61it/s]\u001b[A\n",
      "Loss: 0.122 | AUC: 0.959:  76%|███████▌  | 1140/1500 [07:15<01:18,  4.58it/s]\u001b[A\n",
      "Loss: 0.115 | AUC: 0.963:  76%|███████▌  | 1140/1500 [07:15<01:18,  4.58it/s]\u001b[A\n",
      "Loss: 0.115 | AUC: 0.963:  78%|███████▊  | 1170/1500 [07:17<00:58,  5.64it/s]\u001b[A\n",
      "Loss: 0.114 | AUC: 0.963:  78%|███████▊  | 1170/1500 [07:17<00:58,  5.64it/s]\u001b[A\n",
      "Loss: 0.114 | AUC: 0.963:  80%|████████  | 1200/1500 [07:20<00:44,  6.74it/s]\u001b[A\n",
      "Loss: 0.116 | AUC: 0.962:  80%|████████  | 1200/1500 [07:20<00:44,  6.74it/s]\u001b[A\n",
      "Loss: 0.116 | AUC: 0.962:  82%|████████▏ | 1230/1500 [07:22<00:34,  7.76it/s]\u001b[A\n",
      "Loss: 0.117 | AUC: 0.961:  82%|████████▏ | 1230/1500 [07:22<00:34,  7.76it/s]\u001b[A\n",
      "Loss: 0.117 | AUC: 0.961:  84%|████████▍ | 1260/1500 [09:16<04:54,  1.23s/it]\u001b[A\n",
      "Loss: 0.113 | AUC: 0.963:  84%|████████▍ | 1260/1500 [09:16<04:54,  1.23s/it]\u001b[A\n",
      "Loss: 0.113 | AUC: 0.963:  86%|████████▌ | 1290/1500 [09:18<03:05,  1.13it/s]\u001b[A\n",
      "Loss: 0.112 | AUC: 0.964:  86%|████████▌ | 1290/1500 [09:18<03:05,  1.13it/s]\u001b[A\n",
      "Loss: 0.112 | AUC: 0.964:  88%|████████▊ | 1320/1500 [09:24<02:01,  1.48it/s]\u001b[A\n",
      "Loss: 0.110 | AUC: 0.964:  88%|████████▊ | 1320/1500 [09:24<02:01,  1.48it/s]\u001b[A\n",
      "Loss: 0.110 | AUC: 0.964:  90%|█████████ | 1350/1500 [09:31<01:22,  1.83it/s]\u001b[A\n",
      "Loss: 0.115 | AUC: 0.963:  90%|█████████ | 1350/1500 [09:31<01:22,  1.83it/s]\u001b[A\n",
      "Loss: 0.115 | AUC: 0.963:  92%|█████████▏| 1380/1500 [09:39<00:55,  2.18it/s]\u001b[A\n",
      "Loss: 0.114 | AUC: 0.964:  92%|█████████▏| 1380/1500 [09:39<00:55,  2.18it/s]\u001b[A\n",
      "Loss: 0.114 | AUC: 0.964:  94%|█████████▍| 1410/1500 [09:47<00:36,  2.49it/s]\u001b[A\n",
      "Loss: 0.117 | AUC: 0.962:  94%|█████████▍| 1410/1500 [09:47<00:36,  2.49it/s]\u001b[A\n",
      "Loss: 0.117 | AUC: 0.962:  96%|█████████▌| 1440/1500 [09:54<00:21,  2.85it/s]\u001b[A\n",
      "Loss: 0.109 | AUC: 0.965:  96%|█████████▌| 1440/1500 [09:54<00:21,  2.85it/s]\u001b[A\n",
      "Loss: 0.109 | AUC: 0.965:  98%|█████████▊| 1470/1500 [10:01<00:09,  3.14it/s]\u001b[A\n",
      "Loss: 0.114 | AUC: 0.964:  98%|█████████▊| 1470/1500 [10:01<00:09,  3.14it/s]\u001b[A\n",
      "Loss: 0.114 | AUC: 0.964: 100%|██████████| 1500/1500 [12:43<00:00,  1.84s/it]\u001b[A\n",
      "Loss: 0.111 | AUC: 0.964: 100%|██████████| 1500/1500 [12:43<00:00,  1.84s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "#------------------\n",
    "# GRAPH EXECUTION\n",
    "#------------------\n",
    "\n",
    "# Run the session. \n",
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "# This has noting to do with tensorflow but gives\n",
    "# us a nice progress bar for the training.\n",
    "progress = tqdm(total=batches*epochs)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for _ in range(batches):\n",
    "\n",
    "        # We want to sample one known and one unknown \n",
    "        # item for each user. \n",
    "\n",
    "        # First we sample 15000 uniform indices.\n",
    "        idx = np.random.randint(low=0, high=len(uids), size=samples)\n",
    "\n",
    "        # We then grab the users matching those indices.\n",
    "        batch_u = uids[idx].reshape(-1, 1)\n",
    "\n",
    "        # Then the known items for those users.\n",
    "        batch_i = iids[idx].reshape(-1, 1)\n",
    "\n",
    "        # Lastly we randomly sample one unknown item for each user.\n",
    "        batch_j = np.random.randint(\n",
    "                low=0, high=len(movies), size=(samples, 1), dtype='int32')\n",
    "\n",
    "        # Feed our users, known and unknown items to\n",
    "        # our tensorflow graph. \n",
    "        feed_dict = { u: batch_u, i: batch_i, j: batch_j }\n",
    "\n",
    "        # We run the session.\n",
    "        _, l, auc = session.run([step, loss, u_auc], feed_dict)\n",
    "\n",
    "    progress.update(batches)\n",
    "    progress.set_description('Loss: %.3f | AUC: %.3f' % (l, auc))\n",
    "\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
