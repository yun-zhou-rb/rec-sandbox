{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR: Bayesian Personalized Ranking from Implicit Feedback\n",
    "\n",
    "Ref: \n",
    "* https://arxiv.org/pdf/1205.2618\n",
    "* https://medium.com/radon-dev/implicit-bayesian-personalized-ranking-in-tensorflow-b4dfa733c478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# LOAD AND PREPARE THE DATA\n",
    "#---------------------------\n",
    "\n",
    "# Load the dataframe from a tab separated file.\n",
    "df = pd.read_csv('data/movielens/ml-latest-small/ratings.csv', sep=',')\n",
    "    \n",
    "# Add column names\n",
    "df = df.drop(df.columns[3], axis=1)\n",
    "df_movie = pd.read_csv('data/movielens/ml-latest-small/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_encoder = LabelEncoder()\n",
    "df_movie['movie_id']=movie_encoder.fit_transform(df_movie.movieId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  movie_id  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy         0  \n",
       "1                   Adventure|Children|Fantasy         1  \n",
       "2                               Comedy|Romance         2  \n",
       "3                         Comedy|Drama|Romance         3  \n",
       "4                                       Comedy         4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#users: 610, #items: 9,742 #ratings: 100,836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop any rows with 0 rating\n",
    "df = df.loc[df.rating != 0]\n",
    "\n",
    "# Convert movies into numerical IDs\n",
    "df['user_id'] = df['userId'].astype(\"category\").cat.codes\n",
    "df['movie_id'] = movie_encoder.transform(df['movieId'].values)\n",
    "\n",
    "# Create a lookup frame so we can get the movie\n",
    "# names back in readable form later.\n",
    "item_lookup = df[['movie_id', 'movieId']].drop_duplicates()\n",
    "item_lookup['movie_id'] = item_lookup.movie_id.astype(str)\n",
    "\n",
    "# We drop our old user and item columns\n",
    "df = df.drop(['userId', 'movieId'], axis=1)\n",
    "\n",
    "# Drop any rows with 0 rating\n",
    "df = df.loc[df.rating != 0]\n",
    "\n",
    "# Create lists of all users, movies and ratings\n",
    "users = list(np.sort(df.user_id.unique()))\n",
    "movies = list(np.sort(df_movie.movie_id.unique()))\n",
    "ratings = list(df.rating)\n",
    "print(f\"#users: {len(users):,}, #items: {len(movies):,} #ratings: {len(ratings):,}\" )\n",
    "\n",
    "# Get the rows and columns for our new matrix\n",
    "rows = df.user_id.astype(float)\n",
    "cols = df.movie_id.astype(float)\n",
    "\n",
    "# Contruct a sparse matrix for our users and items containing number of ratings\n",
    "data_sparse = sp.csr_matrix((ratings, (rows, cols)), shape=(len(users), len(movies)))\n",
    "\n",
    "# Get the values of our matrix as a list of user ids\n",
    "# and item ids. Note that our litsts have the same length\n",
    "# as each user id repeats one time for each rated movie.\n",
    "uids, iids = data_sparse.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9742, 9269) (9742, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yunzhou/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "title_vectorizer = TfidfVectorizer()\n",
    "movie_titles = title_vectorizer.fit_transform(df_movie.title)\n",
    "genre_vectorizer = TfidfVectorizer()\n",
    "movie_genre = genre_vectorizer.fit_transform(df_movie.genres)\n",
    "print(movie_titles.shape, movie_genre.shape)\n",
    "#movie_features = sp.hstack((movie_titles,movie_genre))\n",
    "movie_features = movie_genre\n",
    "movie_features = movie_features.tocsr()\n",
    "movie_item_features = movie_features[iids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9742x24 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 23219 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "# HYPERPARAMS\n",
    "#-------------\n",
    "\n",
    "epochs = 50\n",
    "# How many (u,i,j) triplets we sample for each batch\n",
    "samples = 150\n",
    "batches = math.ceil(uids.shape[0]/samples)\n",
    "# batches = min(batches,30)\n",
    "num_factors = 64 # Number of latent features\n",
    "\n",
    "# Independent lambda regularization values \n",
    "# for user, items and bias.\n",
    "lambda_user = 0.0000001\n",
    "lambda_item = 0.0000001\n",
    "lambda_bias = 0.0000001\n",
    "\n",
    "# Our learning rate \n",
    "lr = 0.005\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# TENSORFLOW GRAPH\n",
    "#-------------------------\n",
    "\n",
    "# Set up our Tensorflow graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "def init_variable(size, dim, name=None):\n",
    "    '''\n",
    "    Helper function to initialize a new variable with\n",
    "    uniform random values.\n",
    "    '''\n",
    "    std = np.sqrt(2 / dim)\n",
    "    return tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n",
    "\n",
    "\n",
    "def get_variable(graph, session, name):\n",
    "    '''\n",
    "    Helper function to get the value of a\n",
    "    Tensorflow variable by name.\n",
    "    '''\n",
    "    v = graph.get_operation_by_name(name)\n",
    "    v = v.values()[0]\n",
    "    v = v.eval(session=session)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "#     coo = X.tocoo()\n",
    "#     indices = np.mat([coo.row, coo.col]).transpose()\n",
    "#     return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sparse_tensor(row,col,value,shape):\n",
    "    indices = np.mat([row, col]).transpose()\n",
    "    return tf.SparseTensor(indices, value, shape)\n",
    "\n",
    "def map_one_user_item_pair(u,i,j,i_item_indices,i_item_values,j_item_indices,j_item_values):\n",
    "    \n",
    "    item_shape=[tf.size(i),movie_features.shape[1]]\n",
    "    return (u,i,j,\n",
    "            tf.SparseTensor(i_item_indices, i_item_values,item_shape),\n",
    "            tf.SparseTensor(j_item_indices, j_item_values,item_shape))\n",
    "\n",
    "def gen():\n",
    "    for u,i in zip(uids, iids):\n",
    "        j = np.random.randint(0, len(movies))\n",
    "        i_item_row,i_item_col = movie_item_features[i].nonzero()\n",
    "        i_item_values = movie_item_features[i].data\n",
    "        j_item_row, j_item_col = movie_item_features[j].nonzero()\n",
    "        j_item_values = movie_item_features[j].data\n",
    "        i_item_indices=np.mat([i_item_row, i_item_col]).transpose()\n",
    "        j_item_indices=np.mat([j_item_row, j_item_col]).transpose()\n",
    "        yield (u,i,j,i_item_indices,i_item_values,j_item_indices,j_item_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 07:11:38.524363 139730583705408 deprecation.py:323] From /home/yunzhou/anaconda3/envs/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W0830 07:11:38.556321 139730583705408 deprecation.py:323] From <ipython-input-12-9309ceb3b7b3>:20: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Loss function: \n",
    "    -SUM ln σ(xui - xuj) + λ(w1)**2 + λ(w2)**2 + λ(w3)**2 ...\n",
    "    ln = the natural log\n",
    "    σ(xuij) = the sigmoid function of xuij.\n",
    "    λ = lambda regularization value.\n",
    "    ||W||**2 = the squared L2 norm of our model parameters.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    train_data = tf.data.Dataset.from_generator(gen,\n",
    "                                                (tf.int32, tf.int32, tf.int32,\n",
    "                                                 tf.int64, tf.float32,\n",
    "                                                 tf.int64, tf.float32))\n",
    "\n",
    "    train_data = train_data.map(map_one_user_item_pair)\n",
    "    #train_data = train_data.batch(samples)\n",
    "    train_data = train_data.prefetch(4)\n",
    "    iterator = train_data.make_initializable_iterator()\n",
    "    u,i,j,i_features,j_features = iterator.get_next()\n",
    "    \n",
    "    # User feature embedding\n",
    "    user_factors = init_variable(len(users), num_factors, \"user_factors\") # V matrix\n",
    "    u_factors = tf.nn.embedding_lookup(user_factors, u)\n",
    "    # Known and unknown item embeddings\n",
    "    item_factors = init_variable(len(movies), num_factors, \"item_factors\") # V matrix\n",
    "    item_feature_weights = init_variable(movie_features.shape[1], num_factors, \"item_feature_weights\") # V matrix\n",
    "    i_identity_factors = tf.nn.embedding_lookup(item_factors, i)\n",
    "    j_identity_factors = tf.nn.embedding_lookup(item_factors, j)\n",
    "    i_features_factors = tf.sparse.sparse_dense_matmul(i_features,item_feature_weights)\n",
    "    j_features_factors = tf.sparse.sparse_dense_matmul(j_features,item_feature_weights)\n",
    "    \n",
    "    i_factors = i_identity_factors + i_features_factors\n",
    "    j_factors = j_identity_factors + j_features_factors\n",
    "\n",
    "    # i and j bias embeddings.\n",
    "    item_bias = init_variable(len(movies), 1, \"item_bias\")\n",
    "    i_bias = tf.nn.embedding_lookup(item_bias, i)\n",
    "    i_bias = tf.squeeze(i_bias)\n",
    "    j_bias = tf.nn.embedding_lookup(item_bias, j)\n",
    "    j_bias = tf.squeeze(j_bias)\n",
    "\n",
    "    # Calculate the dot product + bias for known and unknown\n",
    "    # item to get xui and xuj.\n",
    "    ui = tf.reduce_sum(u_factors * i_factors, axis=1)\n",
    "    xui = i_bias + ui\n",
    "    uj = tf.reduce_sum(u_factors * j_factors, axis=1)\n",
    "    xuj = j_bias + uj\n",
    "\n",
    "    # We calculate xuij.\n",
    "    xuij = xui - xuj\n",
    "\n",
    "    # Calculate the mean AUC (area under curve).\n",
    "    # if xuij is greater than 0, that means that \n",
    "    # xui is greater than xuj (and thats what we want).\n",
    "    u_auc = tf.reduce_mean(tf.cast(xuij > 0,tf.float32))\n",
    "\n",
    "    # Output the AUC value to tensorboard for monitoring.\n",
    "    tf.summary.scalar('auc', u_auc)\n",
    "\n",
    "    # Calculate the squared L2 norm ||W||**2 multiplied by λ.\n",
    "    l2_norm = tf.add_n([\n",
    "        lambda_user * tf.reduce_sum(tf.multiply(u_factors, u_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(i_factors, i_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(j_factors, j_factors)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(i_bias, i_bias)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(j_bias, j_bias))\n",
    "        ])\n",
    "\n",
    "    # Calculate the loss as ||W||**2 - ln σ(Xuij)\n",
    "    #loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij)))\n",
    "    loss = -tf.reduce_mean(tf.log(tf.sigmoid(xuij))) + l2_norm\n",
    "    \n",
    "    # Train using the Adam optimizer to minimize \n",
    "    # our loss function.\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    step = opt.minimize(loss)\n",
    "\n",
    "    # Initialize all tensorflow variables.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.400 | AUC: 1.000: 100%|██████████| 33650/33650 [00:00<00:00, 43410.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.31 s, sys: 242 ms, total: 1.55 s\n",
      "Wall time: 1.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#------------------\n",
    "# GRAPH EXECUTION\n",
    "#------------------\n",
    "\n",
    "# Run the session. \n",
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "# This has noting to do with tensorflow but gives\n",
    "# us a nice progress bar for the training.\n",
    "progress = tqdm(total=batches*epochs)\n",
    "idx=np.arange(uids.shape[0])\n",
    "for _ in range(epochs):\n",
    "    session.run(iterator.initializer)\n",
    "\n",
    "    # We run the session.\n",
    "    _, l, auc = session.run([step, loss, u_auc])\n",
    "    progress.update(batches)\n",
    "    progress.set_description('Loss: %.3f | AUC: %.3f' % (l, auc))\n",
    "\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch tensors with different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 07:11:40.152039 139730583705408 deprecation.py:323] From <ipython-input-14-9afc6f372a74>:29: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1 1]\n",
      " [2 2]]\n",
      "1\n",
      "[[3 3]\n",
      " [4 4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://stackoverflow.com/questions/49531286/tensorflow-tf-data-dataset-cannot-batch-tensors-with-different-shapes-in-compo\n",
    "import tensorflow as tf\n",
    "def gen():\n",
    "    for i in range(1, 5):\n",
    "        yield [i, i]\n",
    "\n",
    "def get_batch_gen(gen, batch_size=2):\n",
    "    def batch_gen():\n",
    "        buff = []\n",
    "        for i, x in enumerate(gen()):\n",
    "            if i % batch_size == 0 and buff:\n",
    "                #yield np.concatenate(buff, axis=0)\n",
    "                yield buff\n",
    "                buff = []\n",
    "            buff += [x]\n",
    "\n",
    "        if buff:\n",
    "            #yield np.concatenate(buff, axis=0)\n",
    "            yield buff\n",
    "\n",
    "    return batch_gen\n",
    "\n",
    "# Create dataset from generator\n",
    "batch_size = 2\n",
    "dataset = tf.data.Dataset.from_generator(get_batch_gen(gen, batch_size),\n",
    "                                         tf.int64, None)\n",
    "\n",
    "# Create iterator from dataset\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "x = iterator.get_next()  # shape (None,)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(2):\n",
    "        print(i)\n",
    "        print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_one_user_item_pair(u,i,j,i_item_indices,i_item_values,j_item_indices,j_item_values):\n",
    "    \n",
    "    item_shape=[tf.size(i),movie_features.shape[1]]\n",
    "    print(tf.size(i))\n",
    "    return (u,i,j,\n",
    "            tf.SparseTensor(i_item_indices, i_item_values,item_shape),\n",
    "            tf.SparseTensor(j_item_indices, j_item_values,item_shape))\n",
    "\n",
    "\n",
    "    \n",
    "def gen_user_item():\n",
    "    for u,i in zip(uids, iids):\n",
    "        yield (u,i)\n",
    "        \n",
    "def gen_batch_from_buff(buff):\n",
    "    buff_size=len(buff)\n",
    "    u=[b[0] for b in buff]\n",
    "    i=[b[1] for b in buff]\n",
    "    j=np.random.randint(0, len(movies), buff_size)\n",
    "    i_item_row,i_item_col = movie_item_features[i].nonzero()\n",
    "    i_item_values = movie_item_features[i].data\n",
    "    j_item_row, j_item_col = movie_item_features[j].nonzero()\n",
    "    j_item_values = movie_item_features[j].data\n",
    "    i_item_indices=np.mat([i_item_row, i_item_col]).transpose()\n",
    "    j_item_indices=np.mat([j_item_row, j_item_col]).transpose()\n",
    "    return (u,i,j,i_item_indices,i_item_values,j_item_indices,j_item_values)\n",
    "        \n",
    "def get_batch_gen(gen_user_item=gen_user_item, batch_size=samples):   \n",
    "    def batch_gen():       \n",
    "        buff = []\n",
    "        for i, x in enumerate(gen()):\n",
    "            if i % batch_size == 0 and buff:\n",
    "                yield gen_batch_from_buff(buff)\n",
    "                buff = []\n",
    "            buff += [x]\n",
    "\n",
    "        if buff:\n",
    "            yield gen_batch_from_buff(buff)\n",
    "\n",
    "    return batch_gen\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Size_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Loss function: \n",
    "    -SUM ln σ(xui - xuj) + λ(w1)**2 + λ(w2)**2 + λ(w3)**2 ...\n",
    "    ln = the natural log\n",
    "    σ(xuij) = the sigmoid function of xuij.\n",
    "    λ = lambda regularization value.\n",
    "    ||W||**2 = the squared L2 norm of our model parameters.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    train_data = tf.data.Dataset.from_generator(get_batch_gen(),\n",
    "                                                (tf.int32, tf.int32, tf.int32,\n",
    "                                                 tf.int64, tf.float32,\n",
    "                                                 tf.int64, tf.float32))\n",
    "\n",
    "    train_data = train_data.map(map_one_user_item_pair)\n",
    "    #train_data = train_data.batch(samples)\n",
    "    #train_data = train_data.prefetch(4)\n",
    "    iterator = train_data.make_initializable_iterator()\n",
    "    u,i,j,i_features,j_features = iterator.get_next()\n",
    "    \n",
    "    # User feature embedding\n",
    "    user_factors = init_variable(len(users), num_factors, \"user_factors\") # V matrix\n",
    "    u_factors = tf.nn.embedding_lookup(user_factors, u)\n",
    "    # Known and unknown item embeddings\n",
    "    item_factors = init_variable(len(movies), num_factors, \"item_factors\") # V matrix\n",
    "    item_feature_weights = init_variable(movie_features.shape[1], num_factors, \"item_feature_weights\") # V matrix\n",
    "    i_identity_factors = tf.nn.embedding_lookup(item_factors, i)\n",
    "    j_identity_factors = tf.nn.embedding_lookup(item_factors, j)\n",
    "    i_features_factors = tf.sparse.sparse_dense_matmul(i_features,item_feature_weights)\n",
    "    j_features_factors = tf.sparse.sparse_dense_matmul(j_features,item_feature_weights)\n",
    "    \n",
    "    i_factors = i_identity_factors + i_features_factors\n",
    "    j_factors = j_identity_factors + j_features_factors\n",
    "\n",
    "    # i and j bias embeddings.\n",
    "    item_bias = init_variable(len(movies), 1, \"item_bias\")\n",
    "    i_bias = tf.nn.embedding_lookup(item_bias, i)\n",
    "    i_bias = tf.squeeze(i_bias)\n",
    "    j_bias = tf.nn.embedding_lookup(item_bias, j)\n",
    "    j_bias = tf.squeeze(j_bias)\n",
    "\n",
    "    # Calculate the dot product + bias for known and unknown\n",
    "    # item to get xui and xuj.\n",
    "    ui = tf.reduce_sum(u_factors * i_factors, axis=1)\n",
    "    xui = i_bias + ui\n",
    "    uj = tf.reduce_sum(u_factors * j_factors, axis=1)\n",
    "    xuj = j_bias + uj\n",
    "\n",
    "    # We calculate xuij.\n",
    "    xuij = xui - xuj\n",
    "\n",
    "    # Calculate the mean AUC (area under curve).\n",
    "    # if xuij is greater than 0, that means that \n",
    "    # xui is greater than xuj (and thats what we want).\n",
    "    u_auc = tf.reduce_mean(tf.cast(xuij > 0,tf.float32))\n",
    "\n",
    "    # Output the AUC value to tensorboard for monitoring.\n",
    "    tf.summary.scalar('auc', u_auc)\n",
    "\n",
    "    # Calculate the squared L2 norm ||W||**2 multiplied by λ.\n",
    "    l2_norm = tf.add_n([\n",
    "        lambda_user * tf.reduce_sum(tf.multiply(u_factors, u_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(i_factors, i_factors)),\n",
    "        lambda_item * tf.reduce_sum(tf.multiply(j_factors, j_factors)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(i_bias, i_bias)),\n",
    "        lambda_bias * tf.reduce_sum(tf.multiply(j_bias, j_bias))\n",
    "        ])\n",
    "\n",
    "    # Calculate the loss as ||W||**2 - ln σ(Xuij)\n",
    "    #loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij)))\n",
    "    loss = -tf.reduce_mean(tf.log(tf.sigmoid(xuij))) + l2_norm\n",
    "    \n",
    "    # Train using the Adam optimizer to minimize \n",
    "    # our loss function.\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    step = opt.minimize(loss)\n",
    "\n",
    "    # Initialize all tensorflow variables.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.041 | AUC: 1.000: 100%|██████████| 33650/33650 [00:00<00:00, 46579.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 991 ms, sys: 39.5 ms, total: 1.03 s\n",
      "Wall time: 808 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#------------------\n",
    "# GRAPH EXECUTION\n",
    "#------------------\n",
    "\n",
    "# Run the session. \n",
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "# This has noting to do with tensorflow but gives\n",
    "# us a nice progress bar for the training.\n",
    "progress = tqdm(total=batches*epochs)\n",
    "idx=np.arange(uids.shape[0])\n",
    "for _ in range(epochs):\n",
    "    session.run(iterator.initializer)\n",
    "\n",
    "    # We run the session.\n",
    "    _, l, auc = session.run([step, loss, u_auc])\n",
    "    progress.update(batches)\n",
    "    progress.set_description('Loss: %.3f | AUC: %.3f' % (l, auc))\n",
    "\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
